{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696836bd",
   "metadata": {},
   "source": [
    "# Pipeline Engine — Interactive Demo\n",
    "\n",
    "This notebook walks through every feature of the generic pipeline engine.\n",
    "Each cell is self-contained — run them top to bottom.\n",
    "\n",
    "**No external dependencies** — only the `pipeline/` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0704c7",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135705a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/david/Desktop/projects/Kayba/agentic-context-engine\n",
      "pipeline pkg: pipeline.pipeline\n"
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "from types import MappingProxyType\n",
    "from pathlib import Path\n",
    "\n",
    "# Jupyter already runs an asyncio event loop.  Pipeline.run() calls\n",
    "# asyncio.run() internally, which would fail.  nest_asyncio patches the\n",
    "# loop to allow nested calls.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Walk up from the notebook directory until we find the project root\n",
    "# (identified by containing a `pipeline/` package directory).\n",
    "_here = Path.cwd()\n",
    "_root = _here\n",
    "for _p in [_here] + list(_here.parents):\n",
    "    if (_p / \"pipeline\" / \"__init__.py\").exists():\n",
    "        _root = _p\n",
    "        break\n",
    "sys.path.insert(0, str(_root))\n",
    "\n",
    "# Clear any stale import (ace/pipeline can shadow the top-level pipeline/).\n",
    "if \"pipeline\" in sys.modules:\n",
    "    del sys.modules[\"pipeline\"]\n",
    "\n",
    "from pipeline import (\n",
    "    Pipeline,\n",
    "    Branch,\n",
    "    MergeStrategy,\n",
    "    StepContext,\n",
    "    SampleResult,\n",
    "    PipelineOrderError,\n",
    "    BranchError,\n",
    ")\n",
    "\n",
    "def show(results: list[SampleResult]) -> None:\n",
    "    \"\"\"Pretty-print a list of SampleResult.\"\"\"\n",
    "    for r in results:\n",
    "        tag = \"OK\" if r.error is None else f\"FAIL @ {r.failed_at}\"\n",
    "        print(f\"  [{tag}] sample={r.sample!r}\")\n",
    "        if r.output:\n",
    "            named = {k: getattr(r.output, k) for k in (\"agent_output\", \"environment_result\", \"reflection\") if getattr(r.output, k) is not None}\n",
    "            if named:\n",
    "                print(f\"         fields:   {named}\")\n",
    "            meta = dict(r.output.metadata)\n",
    "            if meta:\n",
    "                print(f\"         metadata: {meta}\")\n",
    "        if r.error:\n",
    "            print(f\"         error:    {r.error}\")\n",
    "\n",
    "print(f\"Project root: {_root}\")\n",
    "print(f\"pipeline pkg: {Pipeline.__module__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62c4a4",
   "metadata": {},
   "source": [
    "## Step Definitions\n",
    "\n",
    "A **step** is any object with:\n",
    "- `requires: frozenset[str]` — metadata keys it reads\n",
    "- `provides: frozenset[str]` — metadata keys it writes\n",
    "- `__call__(ctx: StepContext) -> StepContext`\n",
    "\n",
    "No base class — pure duck typing via `StepProtocol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3802f693",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Tokenize:\n",
    "    \"\"\"Split sample text into words, store token list and count.\"\"\"\n",
    "    requires = frozenset()\n",
    "    provides = frozenset({\"tokens\", \"word_count\"})\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        tokens = str(ctx.sample).split()\n",
    "        print(f\"    [Tokenize]  '{ctx.sample}' → {len(tokens)} tokens\")\n",
    "        return ctx.replace(metadata=MappingProxyType({\n",
    "            **ctx.metadata, \"tokens\": tokens, \"word_count\": len(tokens),\n",
    "        }))\n",
    "\n",
    "\n",
    "class Uppercase:\n",
    "    \"\"\"Uppercase each token. Requires 'tokens' in metadata.\"\"\"\n",
    "    requires = frozenset({\"tokens\"})\n",
    "    provides = frozenset({\"upper_tokens\"})\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        upper = [t.upper() for t in ctx.metadata[\"tokens\"]]\n",
    "        print(f\"    [Uppercase] {ctx.metadata['tokens']} → {upper}\")\n",
    "        return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"upper_tokens\": upper}))\n",
    "\n",
    "\n",
    "class Reverse:\n",
    "    \"\"\"Reverse each token. Designed to run in parallel with Uppercase.\"\"\"\n",
    "    requires = frozenset({\"tokens\"})\n",
    "    provides = frozenset({\"reversed_tokens\"})\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        rev = [t[::-1] for t in ctx.metadata[\"tokens\"]]\n",
    "        print(f\"    [Reverse]   {ctx.metadata['tokens']} → {rev}\")\n",
    "        return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"reversed_tokens\": rev}))\n",
    "\n",
    "\n",
    "class Summarize:\n",
    "    \"\"\"Combine processed metadata into a final agent_output string.\"\"\"\n",
    "    requires = frozenset({\"upper_tokens\", \"reversed_tokens\", \"word_count\"})\n",
    "    provides = frozenset({\"agent_output\"})\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        summary = (\n",
    "            f\"{ctx.metadata['word_count']} words | \"\n",
    "            f\"upper={ctx.metadata['upper_tokens']} | \"\n",
    "            f\"rev={ctx.metadata['reversed_tokens']}\"\n",
    "        )\n",
    "        print(f\"    [Summarize] → {summary}\")\n",
    "        return ctx.replace(agent_output=summary)\n",
    "\n",
    "\n",
    "class Boom:\n",
    "    \"\"\"Always fails — used to demonstrate error handling.\"\"\"\n",
    "    requires = frozenset()\n",
    "    provides = frozenset()\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        raise RuntimeError(f\"Boom on sample={ctx.sample!r}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35014e6",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Basic Linear Pipeline\n",
    "\n",
    "Chain steps with `.then()`. The pipeline infers its **contracts**\n",
    "(`requires` / `provides`) from the step chain automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0814c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline contracts:\n",
      "  requires = frozenset()   ← external inputs the caller must provide\n",
      "  provides = frozenset({'upper_tokens', 'tokens', 'word_count'})   ← everything the pipeline writes\n",
      "\n",
      "    [Tokenize]  'hello world' → 2 tokens\n",
      "    [Uppercase] ['hello', 'world'] → ['HELLO', 'WORLD']\n",
      "    [Tokenize]  'pipeline engine demo' → 3 tokens\n",
      "    [Uppercase] ['pipeline', 'engine', 'demo'] → ['PIPELINE', 'ENGINE', 'DEMO']\n",
      "  [OK] sample='hello world'\n",
      "         metadata: {'tokens': ['hello', 'world'], 'word_count': 2, 'upper_tokens': ['HELLO', 'WORLD']}\n",
      "  [OK] sample='pipeline engine demo'\n",
      "         metadata: {'tokens': ['pipeline', 'engine', 'demo'], 'word_count': 3, 'upper_tokens': ['PIPELINE', 'ENGINE', 'DEMO']}\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline().then(Tokenize()).then(Uppercase())\n",
    "\n",
    "print(\"Pipeline contracts:\")\n",
    "print(f\"  requires = {pipe.requires}   ← external inputs the caller must provide\")\n",
    "print(f\"  provides = {pipe.provides}   ← everything the pipeline writes\")\n",
    "print()\n",
    "\n",
    "results = pipe.run([\"hello world\", \"pipeline engine demo\"])\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad0e30",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Contract Validation\n",
    "\n",
    "The engine validates step ordering at **construction time**.\n",
    "If a step needs a field that a *later* step provides → `PipelineOrderError`.\n",
    "\n",
    "Fields not provided by *any* step are treated as **external inputs** — no error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f22a4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying: Pipeline().then(Uppercase()).then(Tokenize())\n",
      "\n",
      "  Caught PipelineOrderError:\n",
      "  Uppercase requires {'tokens'} but these are produced by a later step — check step ordering.\n",
      "\n",
      "External input is OK:  requires = frozenset({'tokens'})\n"
     ]
    }
   ],
   "source": [
    "# Wrong order: Uppercase needs 'tokens', but Tokenize comes after\n",
    "print(\"Trying: Pipeline().then(Uppercase()).then(Tokenize())\\n\")\n",
    "\n",
    "try:\n",
    "    Pipeline().then(Uppercase()).then(Tokenize())\n",
    "except PipelineOrderError as e:\n",
    "    print(f\"  Caught PipelineOrderError:\\n  {e}\\n\")\n",
    "\n",
    "# External input: 'tokens' not produced by anyone → valid, caller must provide it\n",
    "p = Pipeline().then(Uppercase())\n",
    "print(f\"External input is OK:  requires = {p.requires}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489099b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Branch — Parallel Fork/Join\n",
    "\n",
    "`.branch()` fans out to N child pipelines in parallel, then **merges**\n",
    "their outputs back into a single `StepContext`.\n",
    "\n",
    "```\n",
    "                 ┌── Uppercase ──┐\n",
    "  Tokenize ──►──┤               ├──► Summarize\n",
    "                 └── Reverse  ──┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c559cbae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requires = frozenset()\n",
      "provides = frozenset({'upper_tokens', 'reversed_tokens', 'word_count', 'agent_output', 'tokens'})\n",
      "\n",
      "    [Tokenize]  'fork join' → 2 tokens\n",
      "    [Uppercase] ['fork', 'join'] → ['FORK', 'JOIN']\n",
      "    [Reverse]   ['fork', 'join'] → ['krof', 'nioj']\n",
      "    [Summarize] → 2 words | upper=['FORK', 'JOIN'] | rev=['krof', 'nioj']\n",
      "  [OK] sample='fork join'\n",
      "         fields:   {'agent_output': \"2 words | upper=['FORK', 'JOIN'] | rev=['krof', 'nioj']\"}\n",
      "         metadata: {'tokens': ['fork', 'join'], 'word_count': 2, 'upper_tokens': ['FORK', 'JOIN'], 'reversed_tokens': ['krof', 'nioj']}\n"
     ]
    }
   ],
   "source": [
    "pipe = (\n",
    "    Pipeline()\n",
    "    .then(Tokenize())\n",
    "    .branch(\n",
    "        Pipeline().then(Uppercase()),\n",
    "        Pipeline().then(Reverse()),\n",
    "        merge=MergeStrategy.RAISE_ON_CONFLICT,\n",
    "    )\n",
    "    .then(Summarize())\n",
    ")\n",
    "\n",
    "print(f\"requires = {pipe.requires}\")\n",
    "print(f\"provides = {pipe.provides}\\n\")\n",
    "\n",
    "results = pipe.run([\"fork join\"])\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7edad5",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Merge Strategies\n",
    "\n",
    "When branches write the **same named field**, the merge strategy decides what happens:\n",
    "\n",
    "| Strategy | Behaviour |\n",
    "|---|---|\n",
    "| `RAISE_ON_CONFLICT` | `ValueError` if any named field differs (metadata always LWW) |\n",
    "| `LAST_WRITE_WINS` | Last branch's value wins for every field |\n",
    "| `NAMESPACED` | Each branch stored at `metadata[\"branch_N\"]`, no conflict possible |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a49dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteAnswer:\n",
    "    requires = frozenset()\n",
    "    provides = frozenset({\"agent_output\"})\n",
    "    def __init__(self, val: str):\n",
    "        self.val = val\n",
    "    def __call__(self, ctx):\n",
    "        return ctx.replace(agent_output=self.val)\n",
    "\n",
    "ctx = StepContext(sample=\"q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c59d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a) RAISE_ON_CONFLICT with conflict:\n",
      "\n",
      "   Caught ValueError: Branch outputs conflict on fields {'agent_output'}. Use a different merge strategy or ensure branches write disjoint fields.\n"
     ]
    }
   ],
   "source": [
    "# a) RAISE_ON_CONFLICT — two branches write different values → error\n",
    "print(\"a) RAISE_ON_CONFLICT with conflict:\\n\")\n",
    "\n",
    "b = Branch(\n",
    "    Pipeline().then(WriteAnswer(\"yes\")),\n",
    "    Pipeline().then(WriteAnswer(\"no\")),\n",
    "    merge=MergeStrategy.RAISE_ON_CONFLICT,\n",
    ")\n",
    "try:\n",
    "    b(ctx)\n",
    "except ValueError as e:\n",
    "    print(f\"   Caught ValueError: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23d814f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b) LAST_WRITE_WINS:\n",
      "\n",
      "   agent_output = 'no'   ← second branch wins\n"
     ]
    }
   ],
   "source": [
    "# b) LAST_WRITE_WINS — second branch always wins\n",
    "print(\"b) LAST_WRITE_WINS:\\n\")\n",
    "\n",
    "b = Branch(\n",
    "    Pipeline().then(WriteAnswer(\"yes\")),\n",
    "    Pipeline().then(WriteAnswer(\"no\")),\n",
    "    merge=MergeStrategy.LAST_WRITE_WINS,\n",
    ")\n",
    "out = b(ctx)\n",
    "print(f\"   agent_output = {out.agent_output!r}   ← second branch wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac780329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c) NAMESPACED:\n",
      "\n",
      "   agent_output          = 'yes'   ← from first branch\n",
      "   branch_0.agent_output = 'yes'\n",
      "   branch_1.agent_output = 'no'\n"
     ]
    }
   ],
   "source": [
    "# c) NAMESPACED — each branch isolated, accessible via metadata key\n",
    "print(\"c) NAMESPACED:\\n\")\n",
    "\n",
    "b = Branch(\n",
    "    Pipeline().then(WriteAnswer(\"yes\")),\n",
    "    Pipeline().then(WriteAnswer(\"no\")),\n",
    "    merge=MergeStrategy.NAMESPACED,\n",
    ")\n",
    "out = b(ctx)\n",
    "print(f\"   agent_output          = {out.agent_output!r}   ← from first branch\")\n",
    "print(f\"   branch_0.agent_output = {out.metadata['branch_0'].agent_output!r}\")\n",
    "print(f\"   branch_1.agent_output = {out.metadata['branch_1'].agent_output!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626b1c3",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Error Handling\n",
    "\n",
    "Every sample produces a `SampleResult` — nothing is dropped silently.\n",
    "A failing sample sets `error` and `failed_at`; other samples continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "577e2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples fail:\n",
      "\n",
      "    [Tokenize]  'good luck' → 2 tokens\n",
      "  [FAIL @ Boom] sample='good luck'\n",
      "         error:    Boom on sample='good luck'!\n"
     ]
    }
   ],
   "source": [
    "print(\"All samples fail:\\n\")\n",
    "results = Pipeline().then(Tokenize()).then(Boom()).run([\"good luck\"])\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62824a52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed success / failure:\n",
      "\n",
      "    [Tokenize]  'ok' → 1 tokens\n",
      "    [Tokenize]  'bad input' → 2 tokens\n",
      "    [Tokenize]  'fine' → 1 tokens\n",
      "  [OK] sample='ok'\n",
      "         metadata: {'tokens': ['ok'], 'word_count': 1}\n",
      "  [FAIL @ MaybeBoom] sample='bad input'\n",
      "         error:    bad sample!\n",
      "  [OK] sample='fine'\n",
      "         metadata: {'tokens': ['fine'], 'word_count': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mixed success / failure:\\n\")\n",
    "\n",
    "class MaybeBoom:\n",
    "    requires = frozenset()\n",
    "    provides = frozenset()\n",
    "    def __call__(self, ctx):\n",
    "        if \"bad\" in str(ctx.sample):\n",
    "            raise RuntimeError(\"bad sample!\")\n",
    "        return ctx\n",
    "\n",
    "results = Pipeline().then(Tokenize()).then(MaybeBoom()).run([\"ok\", \"bad input\", \"fine\"])\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6261bf1e",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Async Boundary — Fire-and-Forget Background\n",
    "\n",
    "Set `async_boundary = True` on a step. Everything from that step onward\n",
    "runs in a **background thread**. `run()` returns immediately.\n",
    "\n",
    "```\n",
    "  Foreground (fast)         Background (slow)\n",
    "  ┌──────────┐             ┌───────────┐\n",
    "  │ Tokenize │ ──────►──── │ SlowScore │\n",
    "  └──────────┘             └───────────┘\n",
    "       │                        │\n",
    "    run() returns          updated later\n",
    "```\n",
    "\n",
    "Call `wait_for_background()` to join all background threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b84bca63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [Tokenize]  'fast return' → 2 tokens\n",
      "    [Tokenize]  'also fast' → 2 tokens\n",
      "\n",
      "run() returned in 0.004s — background still scoring\n",
      "\n",
      "  [OK] sample='fast return'\n",
      "  [OK] sample='also fast'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [SlowScore] sample='fast return' score=20  (background)\n",
      "    [SlowScore] sample='also fast' score=20  (background)\n"
     ]
    }
   ],
   "source": [
    "class SlowScore:\n",
    "    \"\"\"Expensive scoring step that runs in background.\"\"\"\n",
    "    requires = frozenset()\n",
    "    provides = frozenset({\"score\"})\n",
    "    async_boundary = True\n",
    "    max_workers = 2\n",
    "\n",
    "    def __call__(self, ctx: StepContext) -> StepContext:\n",
    "        time.sleep(0.1)\n",
    "        score = ctx.metadata.get(\"word_count\", 0) * 10\n",
    "        print(f\"    [SlowScore] sample={ctx.sample!r} score={score}  (background)\")\n",
    "        return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"score\": score}))\n",
    "\n",
    "pipe = Pipeline().then(Tokenize()).then(SlowScore())\n",
    "\n",
    "t0 = time.monotonic()\n",
    "results = pipe.run([\"fast return\", \"also fast\"], workers=2)\n",
    "elapsed = time.monotonic() - t0\n",
    "\n",
    "print(f\"\\nrun() returned in {elapsed:.3f}s — background still scoring\\n\")\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db875020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for background...\n",
      "\n",
      "Done!\n",
      "\n",
      "  [OK] sample='fast return'\n",
      "         metadata: {'tokens': ['fast', 'return'], 'word_count': 2, 'score': 20}\n",
      "  [OK] sample='also fast'\n",
      "         metadata: {'tokens': ['also', 'fast'], 'word_count': 2, 'score': 20}\n"
     ]
    }
   ],
   "source": [
    "# Now wait for background to finish and inspect the updated results\n",
    "print(\"Waiting for background...\\n\")\n",
    "pipe.wait_for_background(timeout=5.0)\n",
    "print(\"Done!\\n\")\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3805436",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Nested Pipelines\n",
    "\n",
    "A `Pipeline` satisfies `StepProtocol`, so it can be used as a step\n",
    "inside another pipeline. Contracts are inferred recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f21c83b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner: requires=frozenset(), provides=frozenset({'upper_tokens', 'tokens', 'word_count'})\n",
      "Outer: requires=frozenset(), provides=frozenset({'upper_tokens', 'tokens', 'word_count', 'reversed_tokens'})\n",
      "\n",
      "    [Tokenize]  'nested demo' → 2 tokens\n",
      "    [Uppercase] ['nested', 'demo'] → ['NESTED', 'DEMO']\n",
      "    [Reverse]   ['nested', 'demo'] → ['detsen', 'omed']\n",
      "  [OK] sample='nested demo'\n",
      "         metadata: {'tokens': ['nested', 'demo'], 'word_count': 2, 'upper_tokens': ['NESTED', 'DEMO'], 'reversed_tokens': ['detsen', 'omed']}\n"
     ]
    }
   ],
   "source": [
    "inner = Pipeline().then(Tokenize()).then(Uppercase())\n",
    "outer = Pipeline().then(inner).then(Reverse())\n",
    "\n",
    "print(f\"Inner: requires={inner.requires}, provides={inner.provides}\")\n",
    "print(f\"Outer: requires={outer.requires}, provides={outer.provides}\\n\")\n",
    "\n",
    "results = outer.run([\"nested demo\"])\n",
    "show(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eedb5cd",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Workers — Concurrent Sample Processing\n",
    "\n",
    "The `workers` parameter on `run()` controls how many samples are processed\n",
    "in parallel (via an `asyncio.Semaphore` in the foreground event loop).\n",
    "This is independent of `max_workers` on individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffe86852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  workers=1 : 0.60s\n",
      "  workers=6 : 0.10s\n",
      "  speedup   : 5.9x\n"
     ]
    }
   ],
   "source": [
    "class SlowStep:\n",
    "    requires = frozenset()\n",
    "    provides = frozenset({\"done\"})\n",
    "    def __call__(self, ctx):\n",
    "        time.sleep(0.1)\n",
    "        return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"done\": True}))\n",
    "\n",
    "samples = [f\"s{i}\" for i in range(6)]\n",
    "pipe = Pipeline().then(SlowStep())\n",
    "\n",
    "t0 = time.monotonic()\n",
    "pipe.run(samples, workers=1)\n",
    "seq = time.monotonic() - t0\n",
    "\n",
    "t0 = time.monotonic()\n",
    "pipe.run(samples, workers=6)\n",
    "par = time.monotonic() - t0\n",
    "\n",
    "print(f\"  workers=1 : {seq:.2f}s\")\n",
    "print(f\"  workers=6 : {par:.2f}s\")\n",
    "print(f\"  speedup   : {seq / par:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b426125",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | API |\n",
    "|---|---|\n",
    "| Linear chain | `Pipeline().then(A()).then(B())` |\n",
    "| Contract inference | `pipe.requires`, `pipe.provides` |\n",
    "| Parallel fork/join | `.branch(Pipeline().then(A()), Pipeline().then(B()))` |\n",
    "| Merge control | `merge=MergeStrategy.RAISE_ON_CONFLICT / LAST_WRITE_WINS / NAMESPACED` |\n",
    "| Error isolation | `SampleResult.error`, `SampleResult.failed_at` |\n",
    "| Background execution | `async_boundary = True` on a step class |\n",
    "| Background join | `pipe.wait_for_background(timeout=...)` |\n",
    "| Nesting | Use a `Pipeline` as a step inside another `Pipeline` |\n",
    "| Sample concurrency | `pipe.run(samples, workers=N)` |\n",
    "\n",
    "The pipeline engine is **domain-agnostic** — it knows nothing about ACE.\n",
    "The `ace2/` package will add domain-specific steps (Agent, Evaluate,\n",
    "Reflect, Update) on top of this engine."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "ace-framework (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
